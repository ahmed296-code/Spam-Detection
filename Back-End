#----------------------------------------------------------------------------------------------->Imports
import re
import pandas as pd
import string
import nltk
import joblib
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from flask import Flask
from flask import Flask, request, render_template
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk import pos_tag
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score
#----------------------------------------------------------------------------------------------



#------------------------------------------------------------------------------------------->Text Preprocessing
# Load the dataset
df = pd.read_csv('data.csv')

# Combine 'headline' and 'body' into a single feature
df['text'] = df['Headline'].astype(str) + ' ' + df['Body'].astype(str)

# Data Cleaning
# Convert the text to lowercase
df['text'] = df['text'].str.lower()

# Apply the function to remove URLs from the 'text' column
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

df['text'] = df['text'].apply(remove_urls)

# Remove punctuation
df['text'] = df['text'].str.replace('[^\w\s]', '')

# Check for None values and replace with empty string
df['text'].fillna("", inplace=True)

# Remove rows where 'text' is an empty string
df = df[df['text'] != '']

# Reset the index after removing rows to avoid future errors
df.reset_index(drop=True, inplace=True)

# Tokenization
df['text'] = df['text'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else x)

# Remove stopwords
stop = stopwords.words('english')
df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop])

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to convert NLTK's POS tags to WordNet's format
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

# Function to preprocess and lemmatize text
def lemmatize_text(tokens):
    # Get POS tags for each token
    pos_tags = pos_tag(tokens)
    # Lemmatize each token with its POS tag
    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]
    # Join the lemmatized tokens back into a string
    return ' '.join(lemmatized_tokens)

# Apply the lemmatization function to the dataframe
df['text'] = df['text'].apply(lambda x: lemmatize_text(x) if isinstance(x, list) else x)
#----------------------------------------------------------------------------------------------------------



#---------------------------------------------------------------------------------------------------------->Splitting Dataset
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['Label'], test_size=0.2, random_state=42)
-----------------------------------------------------------------------------------------------------------



#---------------------------------------------------------------------------------------------------------->TF-IDF Vectorizer
# Initialize a TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Transform the text data
X_train_tfidf = tfidf.fit_transform(X_train.astype('U')) 
X_test_tfidf = tfidf.transform(X_test.astype('U'))

# Save the tfidf vectorizer
joblib.dump(tfidf, 'tfidf.pkl')
#---------------------------------------------------------------------------------------------------------



#------------------------------------------------------------------------------------------------------------>HyperParameters & XGBoost
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score

# Define the parameter grid
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3],
    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],
    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],
    'gamma': [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4],
    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
    'reg_alpha': [0, 0.25, 0.5, 0.75, 1],
    'reg_lambda': [0.25, 0.5, 0.75, 1, 1.25, 1.5]
}

# Initialize the XGBClassifier
clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Initialize the RandomizedSearchCV
random_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=40, scoring='accuracy', n_jobs=-1, cv=5, verbose=3)

# Fit the model
random_search.fit(X_train_tfidf, y_train)

import joblib
# Save the model
joblib.dump(random_search, 'model.pkl')


#---------------------------------------------------------------------------------------------------------------------> Flask
import string
from flask import Flask, request, render_template
import joblib
import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet

app = Flask(__name__)

# Load the model and vectorizer
random_search = joblib.load('model.pkl')
tfidf = joblib.load('tfidf.pkl')

# Function to remove URLs
def remove_urls(text):
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    return re.sub(url_pattern, '', text)

# Function to convert NLTK's POS tags to WordNet's format
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST':
        # Get the news text from the form
        news_text = request.form['inputText']

        # Data Cleaning
        # Convert the text to lowercase
        news_text = news_text.lower()

        # Remove URLs
        news_text = remove_urls(news_text)

        # Remove punctuation
        news_text = news_text.translate(str.maketrans('', '', string.punctuation))

        # Tokenization
        news_tokens = word_tokenize(news_text)

        # Remove stopwords
        stop_words = stopwords.words('english')
        news_tokens = [word for word in news_tokens if word not in stop_words]

        # POS Tagging
        news_pos_tags = pos_tag(news_tokens)

        # Lemmatization
        lemmatizer = WordNetLemmatizer()
        news_text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in news_pos_tags])

        # Transform the text data
        news_text_tfidf = tfidf.transform([news_text])

        # Predict on the new text using the best model
        news_pred = random_search.predict(news_text_tfidf)

        # Return the prediction
        result = "Real" if news_pred[0] == 1 else "Fake"
        result_class = result

        # Render the index.html template with the result
        return render_template('index.html', resultt=result, result_class=result_class)

    # If it's a GET request, just render the template without any result
    return render_template('index.html')

if __name__ == '__main__':
    app.run(debug=False)
#--------------------------------------------------------------------------------------------------------
